--- 
layout: post
title: MultiMICO Edge Lab - manuelle Installation
date: 2022-05-27
author: Christian Glahn
tags: 
- technik
- edge computing
- container
- linux
- virtualisierung
- lxd/lxc
- docker swarm
---

Dieser Beitrag erkl√§rt die die *manuelle* Installation eines einfachen Edge-Computing-Clusters mit **Ubuntu Linux**, **LXD/LXC** zur Systemvirtualisierung, **OVS** f√ºr die Netzwerkvirtualisierung und **Docker Swarm** f√ºr die eigentlichen Edge-Anwendungen. Die Installation hat viele Stolperfallen, f√ºr die im Internet keine L√∂sungen beschrieben wurden. Stattdessen finden sich viele Beitr√§ge, die behaupteten, dass sich ein solches Setup nicht umsetzen l√§sst. Dieser Beitrag zeigt schrittweise wie die Installation trotzdem gelingt. 

Dieser Blog-Beitrag ist der Erste in einer Serie, die die verschiedenen Konzepte unseres MultiMICO Edge Computing Clusters beschreibt. Mit den weiterf√ºhrenden Konzepten zur Einrichtung eines Edge Clusters befassen sich andere (zuk√ºnftige) Beitr√§ge in dieser Serie.

## Motivation

Eine unserer Herausforderungen beim Entwickeln von Prototypen von Micro-Services f√ºr das Edge-Computing ist eine flexible Arbeitsumgebung. Wir haben uns dazu ein kleines Experimental-Cluster eingerichtet, damit wir verschiedene Konzepte und Technologien ausprobieren und weiterentwickeln k√∂nnen. 

Unser kleines Edge-Computing-Cluster besteht aus 3 [Intel NUC 10 Computern](https://www.intel.de/content/www/de/de/products/docs/boards-kits/nuc/nuc10-performance-kits-mini-pcs-brief.html). Jeder dieser Computer hat einen Intel i7 Prozessor mit 6 Prozessorkernen, 64GB Ram und eine 2TB SSD. Das ist genug Leistung, um verschiedene Virtualisierungsexperimente durchzuf√ºhren. Normalerweise w√ºrden wir solche Hardware direkt f√ºr einen [Docker Swarm](https://docs.docker.com/engine/swarm/) oder ein [Kubernetes Cluster](https://kubernetes.io) verwenden. F√ºr eine Experimentalumgebung wollen wir aber nicht f√ºr jedes Experiment das ganze Cluster neu aufsetzen. Stattdessen wollen wir unsere Experimentalumgebnungen als *virtuelle Infrastruktur* laufen lassen und diese bei Bedarf starten, unterbrechen und l√∂schen k√∂nnen. 

Eine klassische Virtualisierung im Sinne von [VMWare VSphere](https://www.vmware.com/products/vsphere.html) oder [OpenStack](https://www.openstack.org) ist f√ºr unsere Computer *viel* zu anspruchsvoll. Bei diesen Virtualisierungsumgebungen wird jeder Server vollst√§ndig virtualisiert und/oder es werden zus√§tzliche Computer f√ºr die Cluster-Verwaltung ben√∂tigt. Solche Systeme sind sehr komplex und nicht leicht zu installieren. Es l√§sst sich leicht nachvollziehen, dass solche komplexeren Ans√§tze sehr viel Leistung f√ºr die Virtualisierungsinfrastruktur verbrauchen, die dann der eigentlichen Experimentalumgebung nicht mehr zur Verf√ºgung steht. Wir haben deshalb nach einer weniger anspruchsvollen L√∂sung gesucht und mit [LXD/LXC](https://www.linuxcontainers.org) gefunden.

**LXC** steht f√ºr Linux Containers und wird *Lexi* ausgesprochen. LXC stellt nutzt Sicherheits- und Virtualisierungsfunktionen des Linux-Kernels aus, um "virtuelle Systeme" bereitzustellen, bei denen allerdings keine Hardware simuliert wird. Anstatt Hardware zu simulieren, isoliert der Linux-Kernel einzelne Bereiche so, dass sie sich wie eigene Systeme verhalten. Diese Systeme laufen nur ‚Äúzuf√§llig‚Äù auf der gleichen Hardware. Diese Technik bezeichnet man als *Containerisierung*. LXC-Container werden als System-Container bezeichnet, weil diese Container ein ganzes Betriebssystem virtualisieren. 

Im Gegensatz zur konventionellen Virtualisierung gibt es bei LXC-System-Containern jedoch eine Einschr√§nkung: Alle LXC-Container m√ºssen Linux-Systeme sein. Es ist deshalb nicht m√∂glich, mit LXC Windows oder MacOS zu virtualisieren. Praktisch hat diese Einschr√§nkung keine Bedeutung f√ºr uns, weil Windows und MacOS sich als Edge-Computing-Systeme nur sehr bedingt eignen. 

Die zweite Komponente von *LXD/LXC* ist LXD und wird *Lex-Di* ausgesprochen. Mit **LXD** k√∂nnen verschiedene Virtualisierungstechniken auf dem gleichen Computer kombiniert und √ºber eine einheitliche Schnittstelle verwaltet werden. LXD stellt daf√ºr Befehle zur Verf√ºgung, die die Komplexit√§t der unterschiedlichen Virtualisierungsformen vereinheitlicht und vereinfacht. Damit liessen sich auch andere Betriebssysteme auf dem gleichen Rechner betreiben.

F√ºr das MultiMICO Edge-Computing-Cluster stellt LXD/LXC die Plattform f√ºr die Virtualisierung von Anwendungs-Container-*Systemen*. Zu diesen Systemen geh√∂ren Docker (Swarm) und Kubernetes. 

Bei der Vorbereitung unseres Clusters ist uns aufgefallen, dass die Verbindung der verschiedenen Container-Technologien auf der gleichen Hardware nicht ganz einfach ist und dass die Dokumentation im Internet erhebliche L√ºcken hat bzw. hatte. Mit diesem Blog-Post fasse ich unsere recht umst√§ndliche Reise zu einem funktionsf√§higen Docker-Swarm-Cluster auf Basis von LXC-Containern zusammen. 

## Begriffe

- **Host-System**: Rechner mit Anschl√ºssen und Ein-/Ausschalter und Linux-Betriebssystem.
- **System-Container**: Virtuelles System auf einem Host-System, dass sich ansonsten wie ein Host-System verh√§lt.
- **Cluster**: Verbund von Host-Systemen und/oder System-Containern, die die Arbeit untereinander aufteilen.
- **Docker-Container** oder **Anwendungs-Container**: Virtuelles System zum Ausf√ºhren einer einzelnen Anwendung. 
- **OVS** (Open Virtual Switch): Virtueller Netzwerk-Switch, der alle virtualisierten Komponenten mit dem echten Netzwerk verbindet. 

## Anmerkung

Diese Anleitung ist beim Installieren komplexer Infrastrukturen ineffizient. Sie zeigt die minimalen manuellen Schritte, zur Bereitstellung des Basissystems. Mein Team verwendet diese Anleitung **nicht**, sondern hat weite Teile automatisiert. Das ist aber ein Thema f√ºr einen anderen Blog-Beitrag. 

Diese Anleitung fokussiert auf **Docker Swarm**, weil es sich um einen leicht erlernbaren *Orchestrator* f√ºr Docker-Container handelt. Das ist nicht als Wertung f√ºr Docker Swarm und gegen Kubernetes zu verstehen. 

## Voraussetzungen

Die Anleitung setzt voraus, dass Netzwerkkonfigurationen, Firewall-Einstellungen angepasst, die wichtigen SSH-Schl√ºssel bei GitHub registriert sowie gegebenenfalls DNS- und DHCP-Eintr√§ge vorgenommen wurden. 

Ebenfalls wird vorausgesetzt, dass die Public-Key-Anmeldung an einen SSH-Server und die Bedienung von Linux-Systemen auf der Kommandozeile bekannt sind.

## Die Arbeitsschritte

1. Ubuntu (22.04) downloaden
2. Installationsmedium erstellen
3. Installation von Ubuntu Server auf allen Rechnern
4. Installation zus√§tzlicher Pakete auf allen Rechnern
5. Konfiguration von OVS
6. Konfiguration von LXD/LXC
7. Vorbereiten der System-Container f√ºr den Docker Swarm
8. Bereitstellen der System-Container f√ºr den Docker Swarm
9. System-Container zu einem Docker Swarm verbinden

## 1. Ubuntu downloaden

In diesem Beispiel verwenden wir [Ubuntu 22.04](https://ubuntu.com/download/server) als Betriebssystem f√ºr alle Systeme. Weil unsere NUCs einen Intel i7 Prozessor haben, verwenden wir die ``amd64`` Architektur.

```bash
wget https://releases.ubuntu.com/22.04/ubuntu-22.04-live-server-amd64.iso
``` 

## 2. Installationsmedium erstellen

F√ºr die Installation wird ein USB-Stick erstellt, wie es in der [Ubuntu Installationsanleitung](https://help.ubuntu.com/community/Installation/FromUSBStick) beschrieben wird. 

## 3. Installation von Ubuntu auf allen Rechnern 

Vom Installationsmedium wird ein **minimales** Ubuntu mit `OpenSSH-Server` auf allen Rechnern installiert. Die Installationen sind unabh√§ngig voneinander und k√∂nnen theoretisch gleichzeitig durchgef√ºhrt werden.

- Es hilft, wenn auf allen Rechnern der gleiche Nutzer mit dem gleichen Passwort erstellt wird. 
- Es hilft, wenn die eigenen *offentlichen* [SSH-Schl√ºssel via GitHub](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account) auf dem neuen Rechner installiert werden (Punkt `Import SSH Keys` im Installationsprogramm)
- Es ist wichtig, dass ausser dem OpenSSH-Server *keine* zus√§tzlichen Pakete vom Installationsprogramm installiert werden. Solche Pakete laufen sp√§ter ausschliesslich in unseren System-Containern.

Nach der Installation werden alle Host-Systeme neu gestartet.

## 4. Installation zus√§tzlicher Pakete auf allen Rechnern

Die folgende Software sollte anschliessend auf allen Rechnern installiert werden.

```bash
sudo apt update && sudo apt upgrade
sudo apt install -y openvswitch-switch-dpdk libosinfo-bin 
``` 

Optional kann noch das Programm `petname` installiert werden. Damit lassen sich schnell neue Rechnernamen erzeugen.

```bash
sudo apt install -y petname
```

## 5. Konfiguration von OVS

OVS ist f√ºr unser LXD/LXC Cluster wichtig, weil unsere virtuellen Server sonst nicht richtig ins Netz kommen und sich nicht √ºber die Hardware-Hosts hinweg verbinden k√∂nnen. OVS ersetzt das standardm√§ssig von LXD verwendete *Linux Bridged Network*, mit dem Docker Swarm leider nicht funktioniert. 

**Wichtig** Dieser Schritt muss auf *allen* Host-Systemen wiederholt werden. 

Die OVS-Konfiguration kann **nicht** √ºber eine SSH-Verbindung eingerichtet werden, weil wir die Netzwerkschnittstelle austauschen. Die Konfiguration muss deshalb direkt auf dem jeweiligen Host erfolgen. 

```bash
# neuen virtuellen Switch erstellen
ovs-vsctl add-br ovs0

# das Host-System mit dem Switch verbinden
# Erstellt ein virtuelles mgnt0 Interface f√ºr das Host-System
ovs-vsctl add-port ovs0 mgnt0 -- set interface mgnt0 type=internal

# Den virtuellen Switch mit dem physischen Netzwerk verbinden. 
# Achtung! Falls man mit SSH verbunden ist, bricht die Verbindung ab. 
ovs-vsctl add-port ovs0 eno1

# Ubuntu's netplan Konfiguration anpassen und unser neues mgnt0 Interface einf√ºgen
sed -i -e "s/dhcp4: true/dhcp4: no/"  \
       -e "/^  version: 2/i \\ \\ \\ \\ mgnt0:\\n\\ \\ \\ \\ \\ \\ dhcp4: true\\n\\ \\ \\ \\ \\ \\ macaddress: 00:16:3e:f0:06:42" \
       /etc/netplan/00-installer-config.yaml

# Netplan-Konfiguration √ºbernehmen
netplan apply
```

**WICHTIG** Die MAC-Adresse `00:16:3e:f0:06:42` in der netplan Konfiguration **muss** f√ºr jedes Host-System angepasst werden und **eindeutig** sein. Das Tool `create-hwmac.sh` aus unserem [MultiMICO Cluster Repository](https://github.com/multimico/cluster) erstellt beliebig viele "eindeutige" MAC-Adressen im Adressraum des [XEN-Hypervisors](https://xenproject.org). Das ist in unserem Fall in Ordnung, weil wir diesen Hypervisor in unserem Netzwerk nicht verwenden. 

Nach dem Befehl `netplan apply` steht der Rechner wieder √ºber das Netz bereit. Alle weiteren Schritte k√∂nnen deshalb √ºber eine SSH-Verbindung abgewickelt werden. Das macht das Kopieren und Einf√ºgen der Befehle leichter. 

**Achtung** In Unternehmensnetzwerken kann es sein, dass (unbekannte) virtuelle MAC-Adressen nicht akzeptiert werden. Ein Host bekommt dann vom DHCP-Server keine IP-Adresse zugewiesen. Hier helfen die Netzwerkadministratoren weiter. 

## 6. Konfiguration von LXD/LXC

LXD/LXC ist auf Ubuntu 22.04 immer installiert. Wir m√ºssen es nur noch konfigurieren. Dazu erstellen wir auf allen Host-Systemen eine sog. `preseed.yaml`-Datei mit dem folgenden Inhalt. Dazu kann ein beliebiger Editor verwendet werden (z.B. `vi` oder `nano`).

```yaml
# preseed.yaml
config: {}
cluster: null
networks: []
storage_pools:
- config: 
    source: /var/snap/lxd/common/lxd/storage-pools/default
  description: ""
  name: default
  driver: btrfs
- config: 
    source: /var/snap/lxd/common/lxd/storage-pools/docker
  description: docker-storage
  name: docker
  driver: btrfs

profiles:
- name: default
  config: {}
  description: "Standard Konfiguration"
  devices:
    eth0:
      nictype: bridged
      name: eth0
      parent: ovs0
      type: nic
    root:
      path: /
      pool: default
      type: disk

- name: docker
  config:
    security.nesting: true 
    security.syscalls.intercept.mknod: true 
    security.syscalls.intercept.setxattr: true

    linux.kernel_modules: ip_tables,ip6_tables,iptable_nat,iptable_mangle,bridge,nf_nat,br_netfilter,xfrm_user,xt_conntrack,xt_MASQUERADE,bonding,overlay,netlink_diag,ip_vs,ip_vs_dh,ip_vs_ftp,ip_vs_lblc,ip_vs_lblcr,ip_vs_lc,ip_vs_nq,ip_vs_rr,ip_vs_sed,ip_vs_sh,ip_vs_wlc,ip_vs_wrr
    
    limits.memory.swap: false
    
    limits.memory: 16GB 
    limits.cpu: 2 
  description: "Universelle Docker Swarm Konfiguration"
  devices:
    eth0:
      nictype: bridged
      name: eth0
      parent: ovs0
      type: nic
    root:
      path: /
      pool: docker
      type: disk
``` 

Die Werte in den Zeilen `limits.memory` und `limits.cpu` sollte f√ºr die eigene Host-Ausstattung und Anwendungsszenarien angepasst werden. Die hier vorgeschlagenen Limite folgen der Logik von `e2-highmem-2`-Instanzen der [Google Compute Engine (GCE)](https://cloud.google.com/compute/docs/general-purpose-machines?hl=de#e2-high-mem) und reichen auch f√ºr anspruchsvollere Container-Dienste. 

Anschliessend initialisieren wir LXD mit dieser Konfiguration. 

```bash
cat preseed.yaml | lxd init --preseed
``` 

## 7. Vorbereiten der System-Container f√ºr den Docker Swarm

Wir erstellen auf allen Host-Systemen eine Datei mit dem Namen `cloud-init.cfg`. Diese Datei sollte den folgenden Inhalt haben:

```yaml
#cloud-config

hostname: "${HOSTNAME}"
users: 
  - name: multimico
    passwd: "${CRYPTPASSWD}"
    groups:
      - sudo
      - adm
      - plugdev
      - cdrom
      - docker
    ssh_import_id: 
      - "gh:${GITHUBNAME}" 
    lock_passwd: false
    shell: /bin/bash

network:
  version: 2
  ethernets:
    eth0:
      dhcp: yes
      
apt: 
  preserve_sources_list: true
  sources:
    docker:
      source: "deb [arch=amd64] https://download.docker.com/linux/ubuntu jammy stable"
      keyid: 9DC858229FC7DD38854AE2D88D81803C0EBFCD88
      
package_update: true
package_upgrade: true
package_reboot_if_required: true

packages:
  - docker-ce
  - docker-ce-cli
  - containerd.io
```

Die Datei `cloud-init.cfg` wird von Ubuntu dazu verwendet, um die wichtigsten Konfigurationen des System-Containers automatisch vorzunehmen. In unserem Beispiel weist diese Konfiguration einen neuen System-Container an, den Systemnutzer `multimico` einzurichten und Docker zu installieren. Unser Systemnutzer  wird ausserdem den Gruppen `sudo` und `docker` zugewiesen, damit wir das System sp√§ter leichter administrieren k√∂nnen. Ausserdem weisen wir die Installation mit `ssh_import_id` an, SSH-Schl√ºssel f√ºr die Passwort-freie Anmeldung von GitHub zu laden. 

Mit dieser `cloud-init`-Datei stellen wir sicher, dass alle System-Container identisch konfiguriert sind. 

## 8. Bereitstellen der System-Container f√ºr den Docker Swarm

Mit der Konfiguration k√∂nnen wir die System-Container bereitstellen.

Damit wir feste IP-Adressen vergeben k√∂nnen, legen wir die MAC-Adressen vorher fest. Wir k√∂nnen dann, einen System-Container vollst√§ndig ersetzen und sind sicher, dass der Ersatz-Container die gleiche IP-Adresse erh√§lt wie das alte System. 

Wir benennen unsere Systemcontainer wie folgt. In Klammern stehen die verwendeten MAC-Adressen f√ºr die virtuellen Netzwerk-Interfaces.

- `docker1` (`00:16:3e:4B:49:53`)
- `docker2` (`00:16:3e:C1:87:51`)
- `docker3` (`00:16:3e:EE:A0:46`)
- `docker4` (`00:16:3e:E2:95:2C`)
- `docker5` (`00:16:3e:78:C2:C7`)

**Achtung** Falls die MAC-Adressen f√ºr die Hosts in Schritt 5 zentral registriert werden mussten, dann m√ºssen auch die MAC-Adressen f√ºr die System-Container registriert werden. 

Die Container `docker1` und `docker4` laufen auf Host 1. Die Container `docker2` und `docker5` laufen auf Host 2.  Der Container `docker3` l√§uft auf Host 3. 

Unser Passwort ist in diesem Beispiel `f00b4rista`. Selstverst√§ndlich muss es auf einem Produktivsystem angepasst werden.

Jeweils der erste Befehl (`export GITHUBNAME=phish108`)  muss so angepasst werden, dass anstatt von `phish108` der gew√ºnschte GitHub-Nutzer  eingetragen ist, von welchem die [Anmeldeschl√ºssel f√ºr die passwortfreie Anmeldung](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account) importiert weden sollen. 

Auf Host 1 f√ºhren wir die folgenden Befehle aus: 

```bash
export GITHUBNAME=phish108
export HOSTNAME=docker1 CRYPTPASSWD=$( echo -n "f00b4rista" | openssl passwd -6 -stdin ) 

lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:4B:49:53'
lxc start $HOSTNAME

export HOSTNAME=docker4
lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:E2:95:2C'
lxc start $HOSTNAME
```

Auf Host 2 f√ºhren wir die folgenden Befehle aus: 

```bash
export GITHUBNAME=phish108
export HOSTNAME=docker2 CRYPTPASSWD=$( echo -n "f00b4rista" | openssl passwd -6 -stdin ) 

lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:C1:87:51'
lxc start $HOSTNAME

export HOSTNAME=docker5
lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:78:C2:C7'
lxc start $HOSTNAME
```

Auf Host 3 f√ºhren wir die folgenden Befehle aus: 

```bash
export GITHUBNAME=phish108
export HOSTNAME=docker3 CRYPTPASSWD=$( echo -n "f00b4rista" | openssl passwd -6 -stdin ) 

lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:EE:A0:46'
lxc start $HOSTNAME
```

Je nach Netzwerkbandbreite sind die System-Container nach 1-5 Minuten startklar und auf jedem dieser Systeme l√§uft automatisch ein eigener Docker-Dienst.

Wir k√∂nnen uns nun √ºber SSH mit den System-Containern verbinden. 

## 9. System-Container zu einem Docker Swarm verbinden

Abschliessend m√ºssen wir nur noch [die System-Container zu einem Docker Swarm verbinden](https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/) . Dazu m√ºssen wir mindestens  einen Manager festlegen. Diese Rolle kann jeder unserer f√ºnf System-Container √ºbernehmen. Um es einfach zu halten, wird `docker1` unser Manager-Knoten. 

Auf dem System `docker1` f√ºhren wir also den folgenden Befehl aus: 

```bash
docker swarm init
```

Das Ergebnis sieht dann wie folgt aus. 

```
Swarm initialized: current node (jxq17wttcxceai04mktfofzwj) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-XXX-ZENSIERT-XXX 192.168.1.128:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
```

Die einger√ºckte Zeile mit `docker swarm` am Anfang kopieren wir. Es ist m√∂glich, dass diese Zeile im Terminal auf zwei Zeilen umgebrochen wurde. **Es muss unbedingt die ganze Zeile kopiert werden!** Diesen Befehl f√ºhren wir anschliessend in den anderen System-Containern aus. Dabei k√∂nnen wir die IP-Adresse am Ende des Befehls durch den Rechnernamen unseres Manager-Knotens ersetzen. Das sieht dann ungef√§hr so aus.

```bash
docker swarm join --token SWMTKN-1-XXX-ZENSIERT-XXX docker1
``` 

Wir pr√ºfen den Docker Swarm abschliessend mit:

```bash
docker node ls
```

Das Ergebnis wird nun unsere f√ºnf virtuellen Docker-Knoten zeigen. 

Jetzt haben wir einen funktionierenden Docker Swarm in LXC-Containern. ü•≥
