--- 
layout: post
title: MultiMICO Edge Lab - manuelle Installation
date: 2022-05-27
author: Christian Glahn
tags: 
- technik
- edge computing
- container
- linux
- virtualisierung
- lxd/lxc
- docker swarm
---

Eine unserer Herausforderungen bei der Entwicklung von Prototypen von Micro-Services für das Edge Computing ist eine flexible Arbeitsumgebung. Wir haben uns dazu ein kleines Cluster zum Experimentieren mit verschiedenen Konzepten und Technologien eingerichtet. Unser Cluster basiert auf **Ubuntu Linux**, **LXD/LXC** zur Systemvirtualisierung, **OVS** für die Netzwerkvirtualisierung und **Docker Swarm** für die Edge Anwendungen. Dieser Beitrag gibt einen Schnelleinstieg in die *manuelle* Installation unseres MultiMICO Edge Clusters und überspringt deshalb viele Details. 

Dieser Blog ist der erste in einer Serie, die die verschiedenen Konzepte unseres MultiMICO Edge Computing Clusters beschreibt. Mit den weiterführenden Konzepten zur Einrichtung eines Edge Clusters befassen sich andere (zukünftige) Beiträge in dieser Serie.

## Motivation

Unser Edge Computing Cluster besteht aus 3 [Intel NUC 10 Computern](https://www.intel.de/content/www/de/de/products/docs/boards-kits/nuc/nuc10-performance-kits-mini-pcs-brief.html). Jeder dieser Computer hat einen Intel i7 Prozessor mit 6 Prozessorkernen, 64GB Ram und eine 2TB SSD. Das ist genug Leistung, um verschiedene Virtualisierungsexperimente durchzuführen. Normalerweise würden wir solche Hardware direkt für einen [Docker Swarm](https://docs.docker.com/engine/swarm/) oder ein [Kubernetes Cluster](https://kubernetes.io) verwenden. Für eine Experimentalumgebung wollen wir aber nicht für jedes Experiment das ganze Cluster neu aufsetzen. Stattdessen wollen wir unsere Experimentalumgebnungen als *virtuelle Infrastruktur* laufen lassen und diese bei Bedarf starten, unterbrechen und löschen. 

Eine klassische Virtualisierung im Sinne von [VMWare VSphere](https://www.vmware.com/products/vsphere.html) oder [OpenStack](https://www.openstack.org) ist für unsere Computer *viel* zu anspruchsvoll. Bei diesen Virtualisierungsumgebungen wird jeder Server vollständig virtualisiert und/oder es werden zusätzliche Computer für die Cluster Verwaltung benötigt. Solche Systeme sind sehr komplex und nicht leicht zu installieren. Es lässt sich leicht nachvollziehen, dass solche komplexeren Ansätze sehr viel Leistung für die virtuelle Infrastruktur verbrauchen, die dann der eigentlichen Experimentalumgebung nicht mehr zur Verfügung steht. Wir haben nach eine weniger anspruchsvollen Lösung gesucht und mit [LXD/LXC](https://www.linuxcontainers.org) gefunden.

**LXC** steht für Linux Containers und wird *Lexi* ausgesprochen. LXC stellt nutzt Sicherheits- und Virtualisierungsfunktionen des Linux Kernels aus, um "virtuelle Systeme" bereitzustellen, bei denen keine Hardware simuliert wird. Anstatt Hardware zu simulieren isoliert der Linux Kernel einzelne Bereiche so, dass sie sich wie eigene Systeme verhalten, die zufällig auf der gleichen Hardware laufen. Diese Technik bezeichnet man als *Containerisierung*. LXC-Container werden entsprechend als System-Container bezeichnet, weil diese Container ein ganzes Betriebssystem kapseln. 

Bei LXC-System-Containern gibt es jedoch eine Einschränkung: Alle LXC-Container müssen Linux-Systeme sein. Es ist deshalb nicht möglich, mit LXC Windows oder MacOS zu virtualisieren. Praktisch hat diese Einschränkung keine Bedeutung, weil Windows und MacOS sich als Edge-Computing-Systeme nur sehr bedingt eignen. 

Mit **LXD** können verschiedene Virtualisierungstechniken auf dem gleichen Computer kombiniert werden. LXD wird *Lex-Di* ausgesprochen. LXD stellt dafür Befehle zur Verfügung, die die Komplexität der unterschiedlichen Virtualisierungsformen vereinheitlicht und vereinfacht. Damit liessen sich auch verschiedene Betriebssysteme auf dem gleichen Rechner laufen lassen.

Für das MultiMICO Edge Computing Cluster stellt LXD/LXC die Plattform für die Virtualisierung von Anwendungs-Container-Systemen. Zu diesen Systemen gehört Docker (Swarm) und Kubernetes. Bei der Vorbereitung unseres Clusters ist uns aufgefallen, dass die Verbindung der verschiedenen Container-Technologien nicht ganz einfach ist und dass die Dokumentation im Internet erhebliche Lücken hat bzw. hatte. Mit diesem Blog-Post fasse ich unsere recht umständliche Reise zu einem funktionsfähigen Docker-Swarm-Cluster auf Basis von LXC-Containern zusammen. 

## Begriffe

- **Host-System**: Rechner mit Anschlüssen und Ein-/Ausschalter und Linux-Betriebssystem.
- **System-Container**: Virtuelles System auf einem Host-System, dass sich ansonsten wie ein Host-System verhält.
- **Cluster**: Verbund von Host-Systemen und/oder System-Containern, die die Arbeit untereinander aufteilen.
- **Docker-Container** oder **Anwendungs-Container**: Virtuelles System zum Ausführen einer einzelnen Anwendung. 
- **OVS** (Open Virtual Switch): Virtueller Netzwerk-Switch, der alle virtualisierten Komponenten mit dem echten Netzwerk verbindet. 

## Anmerkung

Diese Anleitung ist ineffizient beim Installieren komplexer Infrastruktur. Sie zeigt die minimalen manuellen Schritte, zur Bereitstellung des Basissystems. Mein Team verwendet diese Anleitung **nicht**, sondern hat weite Teile automatisiert. 

Diese Anleitung fokussiert auf **Docker Swarm**, weil es sich um einen leicher zu erlendenden *Orchestrator* für Docker-Container handelt. Es ist nicht als Wertung zwischen Docker Swarm und Kubernetes zu verstehen. 

## Voraussetzungen

Die Anleitung setzt voraus, dass Netzwerkkonfigurationen, Firewall-Einstellungen angepasst, die wichtigen SSH Schlüssel bei GitHub registriert sowie DNS- und DHCP-Einträge vorgenommen wurden. 

Ebenfalls wird vorausgesetzt, dass die Public-Key Anmeldung an einen SSH-Server und die Bedienung von Linux-Systemen auf der Kommandozeile bekannt ist.

## Die Arbeitsschritte

1. Ubuntu (22.04) downloaden
2. Installationsmedium erstellen
3. Installation von Ubuntu Server auf allen Rechnern
4. Installation zusätzlicher Pakete auf allen Rechnern
5. Konfiguration von OVS
6. Konfiguration von LXD/LXC
7. Vorbereiten der System-Container für den Docker Swarm
8. Bereitstellen der System-Container für den Docker Swarm
9. System-Container zu einem Docker Swarm verbinden

## 1. Ubuntu downloaden

In diesem Beispiel verwenden wir Ubuntu 22.04 als Betriebssystem für alle Systeme. Weil unsere NUCs einen Intel i7 Prozessor haben, verwenden wir die ``amd64`` Architektur.

```bash
wget https://releases.ubuntu.com/22.04/ubuntu-22.04-live-server-amd64.iso
``` 

## 2. Installationsmedium erstellen

Für die Installation wird ein USB-Stick erstellt, wie es in der [Ubuntu Installationsanleitung](https://help.ubuntu.com/community/Installation/FromUSBStick) beschrieben wird. 

## 3. Installation von Ubuntu auf allen Rechnern 

Vom Installationsmedium wird ein ***minimales*** Ubuntu mit `OpenSSH-Server` auf den Rechnern installiert. Die Installationen sind unabhängig voneinander und können theoretischen gleichzeitig durchgeführt werden.

- Es hilft, wenn auf allen Rechnern der gleiche Nutzer mit dem gleichen Passwort erstellt wird. 
- Es hilft, wenn die eigenen *offentlichen* SSH-Schlüssel via GitHub auf dem neuen Rechner installiert werden (Punkt `Import SSH Keys` im Installationsprogramm)
- Es ist wichtig, dass ausser dem OpenSSH-Server *keine* zusätzlichen Pakete instaliert werden. Solche Pakete laufen ausschliesslich in unseren System-Containern.

Nach der Installation werden die Host-Systeme neu gestartet.

## 4. Installation zusätzlicher Pakete auf allen Rechnern

Die folgende Software sollte anschliessend auf allen Rechnern installiert werden.

```bash
sudo apt update && sudo apt upgrade
sudo apt install -y openvswitch-switch-dpdk libosinfo-bin 
``` 

Optional kann noch das Programm `petname` installiert werden. Damit lassen sich schnell neue Rechnernamen erzeugen.

```bash
sudo apt install -y petname
```

## 5. Konfiguration von OVS

OVS ist für unser LXD/LXC Cluster wichtig, weil unsere virtuellen Server sonst nicht richtig ins Netz kommen. OVS ersetzt das standardmässig von LXD verwendete *Linux Bridged Network*, mit dem Docker Swarm nicht funktioniert. 

Diese Konfiguration kann **nicht** über eine ssh-Verbindung eingerichtet werden. Sie muss deshalb am Computer erfolgen. 

```bash
# neuen virtuellen Switch erstellen
ovs-vsctl add-br ovs0

# das Host-System mit dem Switch verbinden
# Erstellt ein virtuelles mgnt0 Interface für das Host-System
ovs-vsctl add-port ovs0 mgnt0 -- set interface mgnt0 type=internal

# Den virtuellen Switch mit dem physischen Netzwerk verbinden. 
# Achtung! Falls man mit SSH verbunden ist, bricht die Verbindung ab. 
ovs-vsctl add-port ovs0 eno1

# Ubuntu's netplan Konfiguration anpassen und unser neues mgnt0 Interface einfügen
sed -i -e "s/dhcp4: true/dhcp4: no/"  \
       -e "/^  version: 2/i \\ \\ \\ \\ mgnt0:\\n\\ \\ \\ \\ \\ \\ dhcp4: true\\n\\ \\ \\ \\ \\ \\ macaddress: 00:16:3e:f0:06:42" \
       /etc/netplan/00-installer-config.yaml

# Netplan-Konfiguration übernehmen
netplan apply
```

***WICHTIG*** Die MAC-Adresse `00:16:3e:f0:06:42` in der netplan Konfiguration **muss** für jedes Host-System angepasst werden und **eindeutig** sein. Das Tool `create-hwmac.sh` aus unserem [MultiMICO Cluster Repository](https://github.com/multimico/cluster) erstellt beliebig viele "eindeutige" MAC-Adressen. 

Nach dem letzten Schritt steht der Rechner wieder über das Netz bereit. **Achtung** In Unternehmensnetzwerken kann es sein, dass die virtuelle Netzwerkadresse nicht akzeptiert wird. Hier helfen die Netzwerkadministratoren weiter. 

## 6. Konfiguration von LXD/LXC

LXD/LXC ist auf Ubuntu 22.04 immer installiert. Wir müssen es nur noch anpassen. Dazu erstellen wir auf allen Host-Systemen eine sog. `preseed.yaml`-Datei mit dem folgenden Inhalt. Dazu kann ein beliebiger Editor verwendet werden (z.B. `vi` oder `nano`).

```yaml
# preseed.yaml
config: {}
cluster: null
networks: []
storage_pools:
- config: 
    source: /var/snap/lxd/common/lxd/storage-pools/default
  description: ""
  name: default
  driver: btrfs
- config: 
    source: /var/snap/lxd/common/lxd/storage-pools/docker
  description: docker-storage
  name: docker
  driver: btrfs

profiles:
- name: default
  config: {}
  description: "Standard Konfiguration"
  devices:
    eth0:
      nictype: bridged
      name: eth0
      parent: ovs0
      type: nic
    root:
      path: /
      pool: default
      type: disk

- name: docker
  config:
    security.nesting: true 
    security.syscalls.intercept.mknod: true 
    security.syscalls.intercept.setxattr: true

    linux.kernel_modules: ip_tables,ip6_tables,iptable_nat,iptable_mangle,bridge,nf_nat,br_netfilter,xfrm_user,xt_conntrack,xt_MASQUERADE,bonding,overlay,netlink_diag,ip_vs,ip_vs_dh,ip_vs_ftp,ip_vs_lblc,ip_vs_lblcr,ip_vs_lc,ip_vs_nq,ip_vs_rr,ip_vs_sed,ip_vs_sh,ip_vs_wlc,ip_vs_wrr
    
    limits.memory.swap: false
    
    limits.memory: 16GB 
    limits.cpu: 2 
  description: "Universelle Docker Swarm Konfiguration"
  devices:
    eth0:
      nictype: bridged
      name: eth0
      parent: ovs0
      type: nic
    root:
      path: /
      pool: docker
      type: disk
``` 

Anschliessend initialisieren wir LXD mit dieser Konfiguration. 

```bash
cat preseed.yaml | lxd init --preseed
``` 

## 7. Vorbereiten der System-Container für den Docker Swarm

Wir erstellen auf allen Host-Systemen eine Datei mit dem Namen `cloud-init.cfg`. Diese Datei sollte den folgenden Inhalt haben:

```yaml
#cloud-config

hostname: "${HOSTNAME}"
users: 
  - name: mulitmico
    passwd: "${CRYPTPASSWD}"
    groups:
      - sudo
      - adm
      - plugdev
      - cdrom
      - docker
    ssh_import_id: 
      - "gh:${GITHUBNAME}" 
    lock_passwd: false
    shell: /bin/bash

network:
  version: 2
  ethernets:
    eth0:
      dhcp: yes
      
apt: 
  preserve_sources_list: true
  sources:
    docker:
      source: "deb [arch=amd64] https://download.docker.com/linux/ubuntu jammy stable"
      keyid: 9DC858229FC7DD38854AE2D88D81803C0EBFCD88
      
package_update: true
package_upgrade: true
package_reboot_if_required: true

packages:
  - docker-ce
  - docker-ce-cli
  - containerd.io
```

Die Datei `cloud-init.cfg` wird von Ubuntu dazu verwendet, um die wichtigsten Konfigurationen des System-Containers automatisch vorzunehmen. In unserem Fall weist diese Datei einen neuen System-Container an, Docker zu installieren.

## 8. Bereitstellen der System-Container für den Docker Swarm

Mit der Konfiguration können wir die System-Container bereitstellen.

Damit wir feste IP-Adressen vergeben können, legen wir die MAC Adressen vorher fest. Wir können dann, einen System-Container vollständig ersetzen und sind sicher, dass der neue Container die gleiche IP-Adresse erhält, wie das alte System. 

Wir benennen unsere Systemcontainer wie folgt. In Klammern stehen die verwendeten MAC-Adressen für die virtuellen Netzwerk-Interfaces.

- `docker1` (`00:16:3e:4B:49:53`)
- `docker2` (`00:16:3e:C1:87:51`)
- `docker3` (`00:16:3e:EE:A0:46`)
- `docker4` (`00:16:3e:E2:95:2C`)
- `docker5` (`00:16:3e:78:C2:C7`)

Die Container `docker1` und `docker4` laufen auf Host 1. Die Container `docker1` und `docker5` laufen auf Host 2. und Die Container `docker3` läuft auf Host 3. 

Unser Passwort ist in diesem Beispiel `f00b4rista`. Selstverständlich muss es auf einem Produktivsystem angepasst werden.

Jeweils der erste Befehl (`export GITHUBNAME=phish108`)  muss jeweils so angepasst werden, dass Anstatt von `phish108` der gewünschte GitHub-Nutzer  eingetragen ist, von welchem die Anmeldeschlüssel für die passwortfreie Anmeldung importiert weden. 

Auf Host 1 führen wir die folgenden Befehle aus: 

```bash
export GITHUBNAME=phish108
export HOSTNAME=docker1 CRYPTPASSWD=$( echo -n "f00b4rista" | openssl passwd -6 -stdin ) 

lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:4B:49:53'
lxc start $HOSTNAME

export HOSTNAME=docker4
lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:E2:95:2C'
lxc start $HOSTNAME
```

Auf Host 2 führen wir die folgenden Befehle aus: 

```bash
export GITHUBNAME=phish108
export HOSTNAME=docker2 CRYPTPASSWD=$( echo -n "f00b4rista" | openssl passwd -6 -stdin ) 

lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:C1:87:51'
lxc start $HOSTNAME

export HOSTNAME=docker5
lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:78:C2:C7'
lxc start $HOSTNAME
```

Auf Host 3 führen wir die folgenden Befehle aus: 

```bash
export GITHUBNAME=phish108
export HOSTNAME=docker3 CRYPTPASSWD=$( echo -n "f00b4rista" | openssl passwd -6 -stdin ) 

lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:EE:A0:46'
lxc start $HOSTNAME
```

Je nach Netzwerkbandbreite sind die System-Container nach 1-5 Minuten startklar und auf jedem dieser Systeme läuft automatisch ein eigenes Docker-Teilsystem.

Wir können uns nun über SSH mit den System-Containern verbinden.

## 9. System-Container zu einem Docker Swarm verbinden

Abschliessend müssen wir nur noch die System-Container zu einem Docker Swarm verbinden. Dazu legen wir einen Manager fest. Um es einfach zu halten, wird `docker1` unser Manager-Knoten. 

Auf dem System `docker1` führen wir also den folgenden Befehl aus: 

```bash
docker swarm init
```

Das Ergebnis sieht dann wie folgt aus. 

```
Swarm initialized: current node (jxq17wttcxceai04mktfofzwj) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-XXX-ZENSIERT-XXX 192.168.1.128:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
```

Wir kopieren nun die eingerückte Zeile mit `docker swarm` am Anfang. Es ist möglich, dass diese Zeile im Terminal auf zwei Zeilen umgebrochen wurde. Es muss unbedingt die ganze Zeile kopiert werden! Diesen Befehl führen wir anschliessend in den anderen System-Containern aus. Dabei können wir die IP-Adresse am Ende des Befehls durch den Rechnernamen unseres Manager-Knotens ersetzen. Das sieht dann ungefähr so aus.

```bash
docker swarm join --token SWMTKN-1-XXX-ZENSIERT-XXX docker1
``` 

Jetzt haben wir einen funktionierenden Docker Swarm in LXC-Containern. 🥳
