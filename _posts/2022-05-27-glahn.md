--- 
layout: post
title: MultiMICO Edge Lab - manuelle Installation
date: 2022-05-27
author: Christian Glahn
tags: 
- technik
- edge computing
- container
- lxd/lxc
- docker swarm
---

Motivation

Unser Edge Computing Lab besteht aus 3 [Intel NUC 10 Computern](https://www.intel.de/content/www/de/de/products/docs/boards-kits/nuc/nuc10-performance-kits-mini-pcs-brief.html). Jeder dieser Computer hat einen Intel i7 Prozessor mit 6 Prozessorkernen, 64GB Ram und eine 2TB SSD. Das ist genug Leistung, um verschiedene Virtualisierungsexperimente durchzuführen

Dieser Blog ist der erste in einer Serie, die die verschiedenen Konzepte unseres MultiMICO Edge Labs beschreibt. Der Beitrag gibt einen Schnelleinstieg in die Architektur und die *manuelle* Installation, überspringt aber viele Details. Mit diesen Details befassen sich andere Beiträge in dieser Serie.

### Begriffe

- **Host-System**: Rechner mit Anschlüssen und Ein-/Ausschalter.
- **System-Container**: Virtuelles System auf einem Host-System, dass sich ansonsten wie ein Host-System verhält.
- **Cluster**: Verbund von Host-Systemen und/oder System-Containern, die die Arbeit untereinander aufteilen.
- **Docker-Container**: Virtuelles System zum Ausführen einer einzelnen Anwendung. 
- **OVS** (Open Virtual Switch): Virtueller Netzwerk-Switch, der alle virtualisierten Komponenten mit dem echten Netzwerk verbindet. 

### Die Arbeitsschritte

1. Ubuntu (22.04) downloaden
2. Installationsmedium erstellen
3. Installation von Ubuntu Server auf allen Rechnern
4. Installation der Host Pakete aur allen Rechnern
5. Konfiguration von OVS
6. Konfiguration von LXD/LXC
7. Vorbereiten der System-Container für den Docker Swarm
8. Bereitstellen der System-Container für den Docker Swarm
9. System-Container zu einem Docker Swarm verbinden

### 1. Ubuntu downloaden

In diesem Beispiel verwenden wir Ubuntu 22.04 als Betriebssystem für alle Systeme. Weil unsere NUCs einen Intel i7 Prozessor haben, verwenden wir die ``amd64`` Architektur.

```bash
wget https://releases.ubuntu.com/22.04/ubuntu-22.04-live-server-amd64.iso
``` 

### 2. Installationsmedium erstellen

Für die Installation wird ein USB-Stick erstellt, wie es in der [Ubuntu Installationsanleitung](https://help.ubuntu.com/community/Installation/FromUSBStick) beschrieben wird. 

### 3. Installation von Ubuntu auf allen Rechnern 

Vom Installationsmedium wird ein *minimales* Ubuntu mit `OpenSSH-Server` nacheinander auf den NUCs installiert. 

- Es hilft, wenn auf allen Rechnern der gleiche Nutzer mit dem gleichen Passwort erstellt wird. 
- Es hilft, wenn die eigenen *offentlichen* SSH-Schlüssel mit GitHub auf den Rechner installieren lässt (Punkt `Import SSH Keys` im Installationsmenu)
- Es ist wichtig, dass neben dem SSH-Server keine weiteren komplexen Packete instaliert werden. 

Nach der Installation werden die Systeme neu gestartet.

### 4. Installation der Host Pakete aur allen Rechnern

Die folgende Software sollte anschliessend auf allen Rechnern installiert werden.

```bash
sudo apt update && sudo apt upgrade
sudo apt install -y openvswitch-switch-dpdk libosinfo-bin 
``` 

Optional kann noch das Programm `petname` installiert werden. Damit lassen sich schnell neue Rechnernamen erzeugen.

```bash
sudo apt install -y petname
```

### 5. Konfiguration von OVS

OVS ist für unser Lab wichtig, weil unsere virtuellen Server so ins Netz kommen. Diese Konfiguration kann **nicht** über eine ssh Verbindung eingerichtet werden. Sie muss deshalb am Computer erfolgen. 

```bash
# neuen virtuellen Switch erstellen
ovs-vsctl add-br ovs0

# das Host-System mit dem Switch verbinden
ovs-vsctl add-port ovs0 mgnt0 -- set interface mgnt0 type=internal

# Den virtuellen Switch mit dem physischen Netzwerk verbinden. 
# Achtung! falls man mit SSH verbunden ist, bricht die Verbindung ab. 
ovs-vsctl add-port ovs0 eno1

# Ubuntu's netplan Konfiguration anpassen,
sed -i -e "s/dhcp4: true/dhcp4: no/"  -e "/^  version: 2/i \\ \\ \\ \\ mgnt0:\\n\\ \\ \\ \\ \\ \\ dhcp4: true\\n\\ \\ \\ \\ \\ \\ macaddress: 00:16:3e:f0:06:42" /etc/netplan/00-installer-config.yaml

# Netplan-Konfiguration übernehmen
netplan apply
```

***WICHTIG*** Die MAC-Adresse `00:16:3e:f0:06:42` **muss** für jedes Host-System angepasst werden und **eindeutig** sein. Das Tool `create-hwmac.sh` aus unserem [MultiMICO Cluster Repository](https://github.com/multimico/cluster) erstellt beliebig viele "eindeutige" MAC-Adressen. 

Nach dem letzten Schritt steht der Rechner wieder über das Netz bereit. **Achtung** In Unternehmensnetzwerken kann es sein, dass die virtuelle Netzwerkadresse nicht akzeptiert wird. Hier helfen die Netzwerkadministratoren weiter. 

### 6. Konfiguration von LXD/LXC

LXD/LXC ist auf Ubuntu 22.04 immer installiert. Wir müssen es nur noch anpassen. Dazu erstellen wir auf allen Host-Systemen eine sog. `preseed.yaml`-Datei mit dem folgenden Inhalt. Dazu kann ein beliebiger Editor verwendet werden (z.B. `vi` oder `nano`).

```yaml
# preseed.yml
config: {}
cluster: null
networks: []
storage_pools:
- config: 
    source: /var/snap/lxd/common/lxd/storage-pools/default
  description: ""
  name: default
  driver: btrfs
- config: 
    source: /var/snap/lxd/common/lxd/storage-pools/docker
  description: docker-storage
  name: docker
  driver: btrfs

profiles:
- name: default
  config: {}
  description: "Standard Konfiguration"
  devices:
    eth0:
      nictype: bridged
      name: eth0
      parent: ovs0
      type: nic
    root:
      path: /
      pool: default
      type: disk

- name: docker
  config:
    security.nesting: true 
    security.syscalls.intercept.mknod: true 
    security.syscalls.intercept.setxattr: true

    linux.kernel_modules: ip_tables,ip6_tables,iptable_nat,iptable_mangle,bridge,nf_nat,br_netfilter,xfrm_user,xt_conntrack,xt_MASQUERADE,bonding,overlay,netlink_diag,ip_vs,ip_vs_dh,ip_vs_ftp,ip_vs_lblc,ip_vs_lblcr,ip_vs_lc,ip_vs_nq,ip_vs_rr,ip_vs_sed,ip_vs_sh,ip_vs_wlc,ip_vs_wrr
    
    limits.memory.swap: false
    
    limits.memory: 16GB 
    limits.cpu: 2 
  description: "Universelle Docker Swarm Konfiguration"
  devices:
    eth0:
      nictype: bridged
      name: eth0
      parent: ovs0
      type: nic
    root:
      path: /
      pool: docker
      type: disk
``` 

Anschliessend initialisieren wir LXD mit dieser Konfiguration. 

```bash
cat preseed.yaml | lxd init --preseed
``` 

### 7. Vorbereiten der System-Container für den Docker Swarm

Wir erstellen auf allen Host-Systemen eine Datei mit dem Namen `cloud-init.cfg`. Diese Datei sollte den folgenden Inhalt haben:

```yaml
#cloud-config

hostname: "${HOSTNAME}"
users: 
  - name: mulitmico
    passwd: "${CRYPTPASSWD}"
    groups:
      - sudo
      - adm
      - plugdev
      - cdrom
      - docker
    ssh_import_id: 
      - "gh:phish108" # Diese Zeile gegen den richtigen GitHub user austauschen, sonst kommt man nicht ins System.
    lock_passwd: false
    shell: /bin/bash

network:
  version: 2
  ethernets:
    eth0:
      dhcp: yes
      
apt: 
  preserve_sources_list: true
  sources:
    docker:
      source: "deb [arch=amd64] https://download.docker.com/linux/ubuntu jammy stable"
      keyid: 9DC858229FC7DD38854AE2D88D81803C0EBFCD88
      
package_update: true
package_upgrade: true
package_reboot_if_required: true

packages:
  - docker-ce
  - docker-ce-cli
  - containerd.io
```

### 8. Bereitstellen der System-Container für den Docker Swarm

Mit der Konfiguration können wir die System-Container bereitstellen.

Damit wir feste IP-Adressen vergeben können, legen wir die MAC Adressen vorher fest. 

Wir nennen unsere Systemcontainer wie folgt. In Klammern steht die verwendeten Mac-Adressen.

- `docker1` (`00:16:3e:4B:49:53`)
- `docker2` (`00:16:3e:C1:87:51`)
- `docker3` (`00:16:3e:EE:A0:46`)
- `docker4` (`00:16:3e:E2:95:2C`)
- `docker5` (`00:16:3e:78:C2:C7`)

Die Container `docker1` und `docker4` laufen auf Host 1. Die Container `docker1` und `docker5` laufen auf Host 2. und Die Container `docker3` läuft auf Host 3. 

Unser Passwort ist in diesem Beispiel `f00b4rista`. Selstverständlich muss es auf einen Produktivsystem angepasst werden.

Auf Host 1 führen wir die folgenden Befehle aus: 

```bash
export HOSTNAME=docker1 CRYPTPASSWD=$( echo -n "f00b4rista" | openssl passwd -6 -stdin )

lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:4B:49:53'
lxc start $HOSTNAME

export HOSTNAME=docker4
lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:E2:95:2C'
lxc start $HOSTNAME
```

Auf Host 2 führen wir die folgenden Befehle aus: 

```bash
export HOSTNAME=docker2 CRYPTPASSWD=$( echo -n "f00b4rista" | openssl passwd -6 -stdin )

lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:C1:87:51'
lxc start $HOSTNAME

export HOSTNAME=docker5
lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:78:C2:C7'
lxc start $HOSTNAME
```

Auf Host 3 führen wir die folgenden Befehle aus: 

```bash
export HOSTNAME=docker3 CRYPTPASSWD=$( echo -n "f00b4rista" | openssl passwd -6 -stdin )

lxc init -p docker ubuntu:22.04 $HOSTNAME
echo "$(cat cloud-init.cfg | envsubst )" | lxc config set $HOSTNAME user.user-data -
lxc config set $HOSTNAME volatile.eth0.hwaddr '00:16:3e:EE:A0:46'
lxc start $HOSTNAME
```

Je nach Netzwerkbandbreite sind die System-Container nach 1-3 Minuten startklar und auf jedem dieser Systeme läuft ein eigenes Docker Teilsystem.

Wir können uns nun über SSH mit den System-Containern verbinden.

### 9. System-Container zu einem Docker Swarm verbinden

Abschliessend müssen wir nur noch die Container zu einem Docker Swarm verbinden. 

Dazu legen wir einen Manager fest. Um es einfach zu halten, wird `docker1` unser Manager-Knoten. 

Auf dem `docker1` Knoten führen wir den folgenden Befehl aus: 

```bash
docker swarm init
```

Das Ergebnis sieht dann wie folgt aus. 

```
Swarm initialized: current node (jxq17wttcxceai04mktfofzwj) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-XXX-REDACTED-XXX 192.168.1.128:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
```

Wir kopieren nun die Zeile mit `docker swarm`  am Anfang. Es ist möglich, dass diese Zeile im Terminal auf zwei Zeilen umgebrochen wurde. Diesen Befehl führen in den anderen System-Containern aus. Dabei können wir die IP-Adresse am Ende des Befehls durch den Rechnernamen unseres Knotens ersetzen. Das sieht dann ungefähr so aus.

```bash
docker swarm join --token SWMTKN-1-XXX-REDACTED-XXX docker1
``` 

Jetzt haben wir einen funktionierenden Docker Swarm mit LXC-Kontainern. 🥳
