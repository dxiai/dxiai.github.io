---
layout: post
title: Ersetzen von For-Schleifen durch Map Reduce.
author: Daniel Bajka
date: 2021-06-27
tags: 
- funktionale programmierung
- prozedurale programmierung
- R
- praxis
- problemzerlegung
---

Wer programmiert, stösst unweigerlich auf die Kontrollstrukrur der Forschleife. Eine der häufigsten Verwendungen ist die [Numerische Schleife](https://de.wikipedia.org/wiki/For-Schleife). Die wichtigsten Merkmale sind: 
```R
For (Zähler in Start : Ende){ 
    ' zu
    ' wiederholende
    ' Anweisungen
    ' bedingte Anweisungen
    ' Abbruchbediungungen
    }
```
- Die Anzahl der Wiederholungen steht schon beim Eintritt in die Schleife fest. 
- Es gibt eine Schleifenvariable, die am Anfang auf den Startwert gesetzt wird und dann jeweils um die Schrittweite verändert wird, bis der Zielwert erreicht ist. 
- Die Schleifenvariable, der Startwert, die Schrittweite und der Endwert müssen numerisch sein. 

Der Komplexität von For-Schleifen sind keine Grenzen gesetzt und damit auch nicht der Unübersichtlichkeit und Fehleranfälligkeit. Unübesichtlich, weil es oftmals viel Zeit erfordert, die Funktionsweise des so erzeugten Codes nachzuvollziehen. Fehleranfällig weil For-Schleifen auch in Funktionen verwendet werden und sowohl mit globalen wie auh lokalen Variablen arbeiten und nicht selten daz verleiten, über Abbruchbedingenen direkt aus der Funktion zu springen, ohne sich der Konsequenzen auf lokale und globale Variablen bewusst zu sein. Dies kann dann zu einem langen und mühsamen debugging führen und einen Projektverlauf unnötig verlangsamen.

Die gute Nachricht ist: Es geht besser, viel besser, mit [`Map Reduce`](). Auf der technischen Ebene liegen die Vorteile vor allem in der Bearbeiten grosser Datenmengen, die automatisch in kleinere Bündel aufgeteilt und dann zeitsparend parallel bearbeitet werden.  
![png](/assets/images/post_20210624/map_reduce.jpeg)


Dazu müssen wir allerdingt das Paradigma der "Prozeduralen Programmierung" verlassen und uns der "Funktionen Programmierung" zuwenden.

https://m.heise.de/developer/artikel/MapReduce-in-Googles-Go-1201324.html?view=zoom&zoom=1


Diese sind Map, Fold (oder Reduce) und Filter. Diese Funktionen abstrahieren die Iteration über Listen, die in einer vektorisierten Sprache wie R allgegenwärtig ist. Eine Karte spiegelt den mathematischen Sinn des Wortes wider, als ein Verfahren, das jedes Element einer Menge durch dieselbe Funktion transformiert. Die Ausgabe von map erfolgt normalerweise eins zu eins mit der Eingabe. Fold hingegen ist ein Mechanismus, um eine Reihe von Daten zu einem einzigen Wert zusammenzufassen. Die Summen- und Produktoperatoren sind gute Beispiele dafür, bei denen ein Vektor auf einen einzigen Wert reduziert wird. Die letzte Funktion gibt eine Teilmenge einer Liste zurück, die auf einem Prädikat basiert. Einige Setzoperationen (z. B. Setdifferenz) sind Beispiele für eine Filteroperation.


Die Normalverteilung ist für die Statistik zentral und viele statistische Methoden setzen voraus, dass die Daten normalverteilt sind. Entsprechend müssen wir immer wieder die Frage beantworten, ob unsere Daten auch hinreichend normalverteilt sind. Diese Frage ist nicht neu und es gibt verschiedene und gut gesicherte Verfahren, um sie zu beantworten. Eine andere nicht so oft gestellte Frage ist, ob unsere Daten überhaupt geeignet sind, um die Normalverteilung statistisch überprüfen zu können. Konkret geht es um die Überprüfung der Normalverteilung, wenn unsere Daten gerundet wurden *und* unsere Daten sehr homogen sind. 

Dazu erinnern wir uns an unsere Statistikeinführung. Dort haben wir sicher gelernt, dass wir nominal-, ordinal- und metrischskalierte Variablen unterscheinden und dass die Normalverteilung nur für metrischskalierte Variablen möglich ist. 
Damit wir diesen Beitrag nachvollziehen können, erinnern wir uns noch, dass wir die Werte von ordinalskalierten Variablen zwar sortieren können aber die Abstände zwischen den Werten unbekannt sind, weil wir nur bestimmte Werte erhalten. Solche Variablen werden auch als *diskret* bezeichnet. Als Beispiel kann uns ein Küchenherd dienen: Bei vielen Elektroherden können wir die Hitze der Kochplatten oder -Felder mit einem Regler steuern. Bei meinem Herd gibt es dafür 10 Stufen von 0 bis 9. Ich kann nur die Stufen 0, 1, 2, 3, 4, 5, 6, 7, 8 und 9 auswählen. Ich weiss zwar, dass 8 heisser als 7 und weniger heiss als 9 ist, aber ich kann nicht beurteilen, ob die Temperaturdifferenz auf den Stufen 7, 8 und 9 immer gleich ist. Bei meinem Herd kann ich auch keine halben oder viertel Stufen einstellen. 

Im Vergleich dazu ist eine metrischskalierte Variable mit einem Gasherd vergleichbar, bei dem wir stufenlos die Hitze einstellen können. Wenn das Stellrad ein gleichmässiges Gewinde hat, dann entspricht eine halbe Umdrehung des Gashahns immer dem gleichen Zuwachs an durchfliessendem Gas und damit mehr Hitze. Zusätzlich können wir die Umdrehungen nach belieben abstufen, wenn wir etwas weniger Hitze haben wollen. Diese Eigenschaften finden wir auch bei anderen Arten von Daten: Geld ist oft metrischskaliert, Entfernungen sind metrischskaliert und auch die Zeit ist metrischskaliert. 

Nun gibt es Daten, die grundsätzlich metrischskaliert sind, wie z.B. die Zeit. In der Praxis reden wir aber je nach Kontext gelegentlich nur über *bestimmte* Zeitintervalle innerhalb dieses kontinuierlichen Spektrums. In solchen Fällen verwenden wir gerundete Werte. Ein Beispiel für solche gerundeten Werte ist das Alter einer Person. Obwohl das Alter ein kontinuierlicher Zeitraum von der Geburt bis zu einem Zeitpunkt ist, sagt praktisch niemand, dass eine Person `39.5678` Jahre alt sei. Vielmehr runden wir diesen Wert zum letzten ganzen Jahr *ab* und stellen fest, dass die Person `39` Jahre alt ist. 

Wir würden erwarten, dass ein solches Abrunden keinen gewaltigen Effekt auf die Verteilung einer beliebigen Gruppe von Personen hat. Das können wir mit R leicht überprüfen. Dazu erstellen wir uns eine Stichprobe mit normalverteilten Werten um den Mittelwert `30` und einer Standardabweichung von `20`.


```R
daten = tibble(o_x = rnorm(101, mean = 30, sd = 20)) 
```

Anschliessend runden wir die Werte und behalten sowohl den originalen ungerundeten als auch den gerundeten Wert. Das Ganze visualisieren wir  mit [ggplot2](https://ggplot2-book.org) als gegenübergestellte Histogramme.


```R
daten %>%
    mutate(r_x = floor(o_x)) -> daten_gerundet

daten_gerundet %>%
    pivot_longer(everything(), names_to = "nm", values_to = "vl") %>%
    ggplot(aes(vl)) +
        geom_histogram(aes(y = ..density..), fill = "white", color = "black" ) +
        geom_line(stat = "function", 
                  fun = dnorm, args= c(mean = 30, sd = 20), color = "red") +
        facet_wrap(vars(nm))
```    

    
![png](/assets/images/post_20210331/output_6_1.png)

Post über die Verwendung von Map Reduce anhand eines Bsp. i welchem eine ANzahl von XML Dateien aus einem lokalen Ordner ausgelesen, gepaarst und in einem Dataframe (TIbble) gespeichert werden. Da dieser Prozess dauern kann, ist ein ProgressBar eingebaut, der nach jedem erfolgreichen Parsing einer Datei, die Anzeige visuell auffrischt.
